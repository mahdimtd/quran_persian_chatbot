# -*- coding: utf-8 -*-
"""Final (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vHb3DoPVQLUH0jtGViLGLGtmyQZ7M8Km
"""

!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface
!pip install chromadb llama-index-vector-stores-chroma
!pip install openai
!pip install gradio

import os
import warnings
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
from huggingface_hub import notebook_login
from openai import OpenAI
import gradio as gr

try:
    notebook_login()
except Exception as e:
    print(f"Hugging Face login failed: {e}")

!wget "https://filedn.eu/l1MYFwJMIh4Y60BIIrYyMiy/Quran_Persian_QA/my_chroma_index%20.zip"

import zipfile

with zipfile.ZipFile('my_chroma_index .zip','r') as zip_ref:
  zip_ref.extractall('my_chroma_index')
print(os.listdir('my_chroma_index'))

PERSIST_DIR = "/content/my_chroma_index/content/my_chroma_index"
EMBED_MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
OPENROUTER_API_KEY = 'your api key'

def load_existing_index():
    """Load the pre-created ChromaDB index"""
    try:
        print("Loading embedding model...")
        embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)

        print("Connecting to ChromaDB...")
        chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)

        # List all collections to see what's available
        collections = chroma_client.list_collections()
        print(f"Found {len(collections)} collections:")
        for col in collections:
            print(f"  - {col.name} (ID: {col.id})")

        if not collections:
            raise Exception("No collections found in the ChromaDB database")

        # Use the first collection (or try to find the right one)
        collection_name = collections[0].name
        print(f"Using collection: {collection_name}")

        chroma_collection = chroma_client.get_collection(name=collection_name)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

        print("Loading vector index...")
        index = VectorStoreIndex.from_vector_store(
            vector_store=vector_store,
            embed_model=embed_model
        )
        print("Index loaded successfully!")
        return index

    except Exception as e:
        print(f"Error loading index: {e}")
        print("\nDebugging information:")
        print(f"Looking for ChromaDB at: {PERSIST_DIR}")

        if os.path.exists(PERSIST_DIR):
            print("✓ ChromaDB directory exists")
            print("Directory contents:")
            for root, dirs, files in os.walk(PERSIST_DIR):
                level = root.replace(PERSIST_DIR, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files:
                    print(f"{subindent}{file}")
        else:
            print("✗ ChromaDB directory not found")

        # Try to reinitialize ChromaDB if it's corrupted
        print("\nTrying to reinitialize ChromaDB...")
        try:
            chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)
            print("ChromaDB client created successfully")
        except Exception as reinit_error:
            print(f"Failed to reinitialize: {reinit_error}")

        raise

def answer_query(query, index, top_k=5):
    """Answer query using RAG with the loaded index"""
    try:
        # Retrieve relevant documents
        retriever = index.as_retriever(similarity_top_k=top_k)
        nodes = retriever.retrieve(query)
        context = "\n\n".join([node.text for node in nodes])

        # Create prompt
        prompt = f"""
تو یک دستیار فارسی زبان هستی که به سوالات بر اساس استناد به قرآن پاسخ جامع و کامل میدهی

اطلاعات:
{context}

سوال:
{query}

اگر سوال کاربر نامرتبط به دین و قران بود بگو که نمیتوانی پاسخ دهی. همچنین مرحله فکر کردنت را به کاربر نشان نده
        """

        # Call OpenRouter API
        from openai import OpenAI
        client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=OPENROUTER_API_KEY,
        )

        completion = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324:free",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=1500
        )

        return completion.choices[0].message.content

    except Exception as e:
        return f"خطا در پاسخ دادن: {e}"

# Load the pre-created index
print("Loading the pre-created index...")

index = load_existing_index()

def interactive_qa():
    """Interactive question-answering session"""
    print("\n" + "="*50)
    print("حالت تعاملی - برای خروج 'quit' تایپ کنید")
    print("="*50)

    while True:
        query = input("\nسوال شما: ")
        if query.lower() in ['quit', 'exit', 'خروج']:
            break

        print("پاسخ:")
        response = answer_query(query, index)
        print(response)

print(interactive_qa())