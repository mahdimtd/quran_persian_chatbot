# -*- coding: utf-8 -*-
"""Preparing_the_database.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19HWImILnqhCAzr1HaYcou5EG1J3Hzi6Z
"""

!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface
!pip install chromadb llama-index-vector-stores-chroma

import os
import warnings
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
from huggingface_hub import notebook_login

try:
    notebook_login()
except Exception as e:
    print(f"Hugging Face login failed: {e}")

def download_file(url, filename):
    """Download file with error handling"""
    try:
        !wget {url} -O {filename}
        print(f"Successfully downloaded {filename}")
        return True
    except Exception as e:
        print(f"Failed to download {filename}: {e}")
        return False

data_files = []
if download_file("https://filedn.eu/l1MYFwJMIh4Y60BIIrYyMiy/Quran_Persian_QA/majmaolbayan.txt", "/content/majmaolbayan.txt"):
    data_files.append("/content/majmaolbayan.txt")

if download_file("https://filedn.eu/l1MYFwJMIh4Y60BIIrYyMiy/Quran_Persian_QA/alborhan.txt", "/content/alborhan.txt"):
    data_files.append("/content/alborhan.txt")

if not data_files:
    raise Exception("No data files were successfully downloaded")

CHUNK_SIZE = 512
CHUNK_OVERLAP = 100
PERSIST_DIR = "/content/my_chroma_index"
COLLECTION_NAME = "my_collection"
EMBED_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

print("Loading documents...")
try:
    documents = SimpleDirectoryReader(input_files=data_files).load_data()
    print(f"Loaded {len(documents)} documents")
except Exception as e:
    print(f"Error loading documents: {e}")
    raise

print("Initializing embedding model...")
try:
    embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME)
    print("Embedding model loaded successfully")
except Exception as e:
    print(f"Error loading embedding model: {e}")
    raise

node_parser = SentenceSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
)

print("Setting up ChromaDB...")
try:
    chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)
    chroma_collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    print("ChromaDB setup complete")
except Exception as e:
    print(f"Error setting up ChromaDB: {e}")
    raise

print("Creating vector index...")
try:
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        embed_model=embed_model,
        node_parser=node_parser,  # Now actually using the node parser
        show_progress=True
    )
    print("Index created successfully")
except Exception as e:
    print(f"Error creating index: {e}")
    raise

print("Creating zip file...")
try:
    !zip -r /content/my_chroma_index.zip /content/my_chroma_index
    print("Zip file created")
except Exception as e:
    print(f"Error creating zip file: {e}")

try:
    from google.colab import files
    files.download('/content/my_chroma_index.zip')
    print("Index downloaded successfully")
except Exception as e:
    print(f"Error downloading file: {e}")

print("Index preparation complete!")